#+TITLE: CSAPP-Cachelab
#+Author: Kangwei Ling
#+DATE: <2016-11-03 Thu>
#+DESCRIPTION: cachelab
#+TAG: :cache:
#+OPTIONS: num:nil ^:nil
You can find my work on this lab at [[https://github.com/kevinkwl/CSAPP-Labs/tree/master/cachelab][Github Repo]].

* Part A: Cache Simulator
To implement a cache simulator, we must accomplish the following jobs:
+ Take command line arguments to setup a cache simulator with specific structure(number of sets, block size, associativity)
+ Implement cache data structure
+ Use traces generated by =Valgrind= to simulate the cache access
+ Implement LRU policy for cache replacement.

** Parse Command Line Arguments
   Use =getopt= function defined in header =getopt.h=. It's very easy to setup it for argument parsing.
#+BEGIN_SRC c
char* opts = "hvs:E:b:t:";
int opt;
while ((opt = getopt(argc, argv, opts)) != -1) {
    ...
}
#+END_SRC

** Cache Data Structure
In this lab, I use doubly-linked list to implement a set in cache, with
additional head and tail node to simplify the insert and delete process.
*** Types
#+BEGIN_SRC c
typedef struct _cache_line
{
    Address tag;
    struct _cache_line *prev;
    struct _cache_line *next;
}CacheLine, *PCacheLine;
#+END_SRC
Here, *Address* is just an alias for =unsigned long long int= type.

*Cache* is defined as follows:
#+BEGIN_SRC c
typedef struct _cache
{
    int n_sets;
    int associativity;
    int index_mask;
    int index_offset;
    int tag_offset;
    PCacheLine *set_head;
    PCacheLine *set_tail;
    int* n_entries;
}Cache, *PCache;
#+END_SRC
The *Cache* data structure store the organization of the cache and some useful
variables to use in the simulation.
+ index_mask :: a mask to compute the set index from addresses.
+ index_offset :: bit length of the block offset part in the address.
+ set_head :: a pointer to an array of cache line pointers (the head nodes)
+ set_tail :: a pointer to tail nodes
+ n_entries :: a pointer to an array that records number of entries of sets.
*** Methods to manipulate the cache
There are 3 procedures to utilize the cache data structure
#+BEGIN_SRC c
void cacheline_lru(Address tag, PCacheLine head, PCacheLine tail);
void cacheline_new(Address tag, PCacheLine head);
void cacheline_visit(PCacheLine line, PCacheLine head);
#+END_SRC
+  cacheline_lru :: simulate LRU process, choose the least recently used (at the
  end of the list), change its tag to the newly loaded(eviction), then insert
  into the head. 
+ cacheline_new :: insert a new line at the start of the set list.
+ cacheline_visit :: under hit circumstance, update the newly visited cache line
     (move it to the start of the set list).
Note that they have some similarities, but they are just simple linked list
manipulations, I don't want bother to do more works.

Also, the initialization and destruction of *Cache* are very important, because
I use dynamic memory. I've defined two functions to handle this part.
#+BEGIN_SRC c
PCache cache_init(int n_idx, int n_blk, int n_assoc);
void cache_deinit(PCache pcache);
#+END_SRC

** Simulation
The simulation part is concerned with the following functions
#+BEGIN_SRC c
void cache_sim(PCache pcache, Address address, Log* log);
void simulate(char *const filename, PCache pcache, Logger* logger);
#+END_SRC
+ cache_sim :: simulate the process of accessing cache with memory address,
     log the hit-miss-eviction status into the *log* struct.
+ simulate :: with the filename, open the trace file, and use the memory access
     traces to call cache_sim

In =simulate=, after each access processed, the =verbose= flag is checked, so
that valuable debug information is available.

** LRU
Using the doubly linked list, it's straightforward to implement the LRU policy.
+ Each time an access hit, that cache line will be moved to start of the linked list.
+ When miss occurs, 
    - if there are still available lines in the set, a new cache line will be inserted at the head of the linked list. 
    - Otherwise, we delete the last node (not tail) from the list and reinsert
      it into the front of the linked list (change its tag to the newly accessing address).

* Optimizing Matrix Transpose
*Blocking* is our resort to optimize matrix transposition. But only relying on
 blocking is not enough, because the requirements are very tight. There are
 certain tricks to do this part.
** 32 x 32
   The Cache is able to store a *8x8* block, so it is good to do matrix block
   transposition of block size *8x8*. This simple optimization will lead to
   300~400 misses.

   By inspection on the trace file using =csim-ref= and do some calculations,
   it's not very hard to find out that many misses arise from transposing the
   diagonal element. The ironic thing is, diagonal elements remain the same
   place after transposition.

   To reduce the number of misses, it's necessary to defer the access of
   diagonal element to the very end of the iteration.
#+BEGIN_SRC c
for (k = i; k < i + 8 && k < N; k++) {
    for (l = j; l < j + 8 && l < M; l++) {
        if (k == l)
            continue;
        B[l][k] = A[k][l];
    }
    if (i == j)
        B[k][k] = A[k][k];
}
#+END_SRC
The inner loops are shown above. This optimization reduce the misses to 287
misses.
** 64 x 64
   This is the hardest part, I've tried many ways, but all failed (1300 is
   really difficult). Every single optimization has to be used. I refered to
   some codes on [[https://github.com/codeworm96/ICS-Labs/tree/master/lab7][Github]] and finally solved this. Point is,
   + use everything you can, use local variable like register, *use B as buffer!*.
   + whenever load something into the cache, get the most out of it
   + whenever a line is in cache, store everything you can.

   I've add some comments in =trans.c= file, the process is straightforward.
   1. use block of size 8
   2. start from the upper 4 lines, transpose the upper left sub matrix, store
      other 4 elements into upper right part of B.
   3. fetch elements from buffered part of B, transpose the lower left part of A, and
      fetch lower right elements, insert lower part of B with element fetched.
   The final result is approximately 1200+ misses.

** 61 x 67
 Same trick from 32 x 32 works here, use bigger block size (18-20).
